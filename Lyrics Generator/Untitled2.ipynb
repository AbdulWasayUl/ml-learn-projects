{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY54T1f26Ko1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "IqA3SCTVAESK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from numpy import random\n",
        "import gensim.downloader as api\n",
        "from PIL import Image\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Dense,Flatten,InputLayer,BatchNormalization,Dropout,Input,LayerNormalization\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2TokenizerFast,create_optimizer,DataCollatorForLanguageModeling,TFGPT2LMHeadModel"
      ],
      "metadata": {
        "id": "74R1ELgS_qwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH=256\n",
        "BATCH_SIZE=6"
      ],
      "metadata": {
        "id": "B0c7cZR9_seb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "Ge9aGs8TAGRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading"
      ],
      "metadata": {
        "id": "CRXqWV1wAI2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d juicobowley/drake-lyrics\n",
        "!unzip \"/content/drake-lyrics.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "xIZlb1Qc_uVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"/content/dataset/drake_data.csv\"\n",
        "dataset = load_dataset('csv', data_files=filepath)"
      ],
      "metadata": {
        "id": "w242lgEZ_vwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "AtTTZfx-_xn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][184]"
      ],
      "metadata": {
        "id": "3UYtaKIo_zeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing"
      ],
      "metadata": {
        "id": "HBt-iMynAN8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"gpt2-medium\"\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "2Ac7Nlz9_1KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][184]"
      ],
      "metadata": {
        "id": "I603jaQL_2pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(example):\n",
        "  try:\n",
        "    outputs = tokenizer(\n",
        "        example[\"lyrics\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "      if length==MAX_LENGTH:\n",
        "        input_batch.append(input_ids)\n",
        "        valid_input_ids=input_ids\n",
        "    if len(input_batch)!=0:\n",
        "      for i in range(BATCH_SIZE-len(input_batch)):\n",
        "        input_batch.append(valid_input_ids)\n",
        "  except:\n",
        "    print(example)\n",
        "    input_batch=[]\n",
        "  return {\"input_ids\": input_batch}"
      ],
      "metadata": {
        "id": "75AM6AOx_4xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset=dataset.map(\n",
        "    preprocess_function,remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "wfiiIuz8AbEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "kS_nUxF8AcxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_out(example):\n",
        "  if len(example['input_ids'])>=1:\n",
        "    return example"
      ],
      "metadata": {
        "id": "eKIy355nAhUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_full_dataset=tokenized_dataset.filter(filter_out)\n",
        "print(tokenized_full_dataset)"
      ],
      "metadata": {
        "id": "m-DEt5YMAi56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "Hp2vGVR1C_m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = tokenized_full_dataset[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\",\"attention_mask\", \"labels\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=1,\n",
        ")"
      ],
      "metadata": {
        "id": "ggjHHTZmDRg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tf_train_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "jYzwHY38DgvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_attention_mask(input):\n",
        "  return {'input_ids':input['input_ids'],\n",
        "          'attention_mask':tf.ones([1,BATCH_SIZE,MAX_LENGTH]),\n",
        "          'labels':input['labels']}"
      ],
      "metadata": {
        "id": "JlixxaM0DhBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=tf_train_dataset.map(adjust_attention_mask)"
      ],
      "metadata": {
        "id": "A8qFGeZHDjog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "1S0UutoHDlf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unbatched_dataset=train_dataset.unbatch()"
      ],
      "metadata": {
        "id": "hWbc33FSExSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in unbatched_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "PC64l-M8Ezbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "s9x-p0iZElF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFGPT2LMHeadModel.from_pretrained(model_id)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TqCf4ET0El3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_steps=len(unbatched_dataset)\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5e-5,\n",
        "    num_warmup_steps=1_000,\n",
        "    num_train_steps=num_train_steps,\n",
        ")\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "TWfwtrJeOc14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(unbatched_dataset, epochs=5)"
      ],
      "metadata": {
        "id": "mUu7LlYbOeYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('/content/drive/MyDrive/nlp/text_generation/gpt2_medium.h5')"
      ],
      "metadata": {
        "id": "dw7uDgY9Of2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "1yKtsxsQPIln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text=\"true love shouldn't be this complicated\""
      ],
      "metadata": {
        "id": "Zm6jPo0TOhhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_text, return_tensors=\"tf\")[\"input_ids\"]"
      ],
      "metadata": {
        "id": "1fqnQj2TOjSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_time=time.time()\n",
        "output_greedy = model.generate(input_ids,max_length=256,do_sample=False)\n",
        "print(tokenizer.decode(output_greedy[0]))\n",
        "print(time.time()-init_time)"
      ],
      "metadata": {
        "id": "U_GKVV9COlAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_time=time.time()\n",
        "output_beam = model.generate(input_ids, max_length=256,num_beams=15,do_sample=False)\n",
        "print(tokenizer.decode(output_beam[0]))\n",
        "print(time.time()-init_time)"
      ],
      "metadata": {
        "id": "I-t19KpROsC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_time=time.time()\n",
        "output_temp = model.generate(input_ids, max_length=256, do_sample=True,temperature=1.0, top_k=0)\n",
        "print(tokenizer.decode(output_temp[0]))\n",
        "print(time.time()-init_time)"
      ],
      "metadata": {
        "id": "5tyltbTTOsar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_time=time.time()\n",
        "output_temp = model.generate(input_ids, max_length=256, do_sample=True,temperature=2.0, top_k=0)\n",
        "print(tokenizer.decode(output_temp[0]))\n",
        "print(time.time()-init_time)"
      ],
      "metadata": {
        "id": "gwgsWOT2Ov1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_time=time.time()\n",
        "output_temp = model.generate(input_ids, max_length=256, do_sample=True,temperature=0.5, top_k=0)\n",
        "print(tokenizer.decode(output_temp[0]))\n",
        "print(time.time()-init_time)"
      ],
      "metadata": {
        "id": "vnc6mB33Oxkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_time=time.time()\n",
        "output_topk = model.generate(input_ids, max_length=256, do_sample=True,temperature=2.0,top_k=50)\n",
        "print(tokenizer.decode(output_topk[0]))\n",
        "print(time.time()-init_time)"
      ],
      "metadata": {
        "id": "4gisk_3_O1MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_time=time.time()\n",
        "output_topp = model.generate(input_ids, max_length=256, do_sample=True,top_p=0.90)\n",
        "print(tokenizer.decode(output_topp[0]))\n",
        "print(time.time()-init_time)"
      ],
      "metadata": {
        "id": "xHZxkoZLO3K4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}