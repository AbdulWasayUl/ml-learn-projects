{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arNY2SjaeWwQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "fbDGtwT7esFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import evaluate\n",
        "import time\n",
        "from numpy import random\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Dense,Flatten,InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from datasets import load_dataset\n",
        "from transformers import create_optimizer,T5TokenizerFast,DataCollatorForSeq2Seq,TFT5ForConditionalGeneration,TFAutoModelForSeq2SeqLM,AutoModelForSeq2SeqLM,TFT5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "0gHZ0uOBeuFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=64\n",
        "MAX_LENGTH=128"
      ],
      "metadata": {
        "id": "_CejaRtEfHYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "MbTUTyvGfKAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading"
      ],
      "metadata": {
        "id": "Oh5pMVU0fpD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id=\"leslyarun/c4_200m_gec_train100k_test25k\""
      ],
      "metadata": {
        "id": "oiprNlF8fLqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_id)"
      ],
      "metadata": {
        "id": "jlkmDv8ZfN6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "Ak6hNCNwfRmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "8lOH3rN6fRzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "U6UxKAXpfsUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"t5-small\"\n",
        "tokenizer=T5TokenizerFast.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "FZKFSfQrfTvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "\n",
        "  inputs = [example for example in examples['input']]\n",
        "  targets = [example for example in examples['output']]\n",
        "\n",
        "  model_inputs = tokenizer(inputs, text_target=targets,max_length=MAX_LENGTH, truncation=True)\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "Ab5zHDbPfVvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset=dataset.map(preprocess_function,batched=True,remove_columns=dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "tFW4mTdIfXCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "5dwSEnYTfZXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset['train'][1000]"
      ],
      "metadata": {
        "id": "ZRvtDZ2bfZ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "MI5z2o7tfce6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=tokenized_dataset[\"train\"].to_tf_dataset(\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "aj001enifeOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset=tokenized_dataset[\"test\"].to_tf_dataset(\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "FHxbGGPdff6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in val_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "CI-HU3wsfhM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "KGRYjx1JfvIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "KAdNxJFafvtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "U4k3H63Qf30i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "num_train_steps=len(train_dataset)*num_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        ")\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "7nNDtNBBf3E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "  train_dataset,\n",
        "  validation_data=val_dataset,\n",
        "  epochs=num_epochs\n",
        ")"
      ],
      "metadata": {
        "id": "TGYZB9b7f6mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "71Gs-t6vf8eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('/content/drive/MyDrive/nlp/gec/t5-small.h5')"
      ],
      "metadata": {
        "id": "WnKAqE2Af_o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "ZeAbqCrZgBax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "c9NFZbwugCiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in val_dataset.take(5):\n",
        "  predictions = model.generate(\n",
        "      input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "  )\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  labels = batch[\"labels\"].numpy()\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  all_preds.extend(decoded_preds)\n",
        "  all_labels.extend(decoded_labels)\n",
        "\n",
        "result = metric.compute(predictions=all_preds, references=all_labels)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "u6EG5ap7gEuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_preds"
      ],
      "metadata": {
        "id": "mQEaAWskgGZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_labels"
      ],
      "metadata": {
        "id": "jy4lDE3ZgIYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "51xtOUk5gMC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_english=[\n",
        "    \"Dady hav'e eateing her foot\",\n",
        "    \"DJ Sorryyouwastedyourmoneytobehere\",\n",
        "    \"i used to like to swimming\",\n",
        "    \"maybe we should organized a meetin with the people from unesco\",\n",
        "    \"when are we goinge to start play football\",\n",
        "    \"many a time rain fall in my city\"\n",
        "    ]\n",
        "tokenized=tokenizer(\n",
        "  wrong_english,\n",
        "  padding=\"longest\",\n",
        "  max_length=MAX_LENGTH,\n",
        "  truncation=True,\n",
        "  return_tensors='tf'\n",
        ")\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "V4P1nIPWgNBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(wrong_english)):\n",
        "  print(wrong_english[i]+\"------------>\"+tokenizer.decode(out[i], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "EOBrFKUIgOw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xa360l4Dgf1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model=AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"juancavallotti/t5-base-gec\"\n",
        ")"
      ],
      "metadata": {
        "id": "3oNQQrVogf4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_english=[\n",
        "    \"Dady hav'e eateing her foot\",\n",
        "    \"DJ Sorryyouwastedyourmoneytobehere\",\n",
        "    \"i used to like to swimming\",\n",
        "    \"maybe we should organized a meetin with the people from unesco\",\n",
        "    \"when are we goinge to start play football\",\n",
        "    \"many a time rain fall in my city\",\n",
        "   ]\n",
        "tokenized=tokenizer(\n",
        "  wrong_english,\n",
        "  padding=\"longest\",\n",
        "  max_length=MAX_LENGTH,\n",
        "  truncation=True,\n",
        "  return_tensors='pt'\n",
        ")\n",
        "out=pretrained_model.generate(**tokenized, max_length=128)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "y98pI7sUggNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(wrong_english)):\n",
        "  print(wrong_english[i]+\"------------>\"+tokenizer.decode(out[i], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "h0l6x0W_ghwE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}